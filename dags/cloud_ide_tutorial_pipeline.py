"""
cloud_ide_tutorial_pipeline
DAG auto-generated by Astro Build.
"""

from airflow.decorators import dag
from astro import sql as aql
from astro.sql.table import Table
import pandas as pd
import pendulum


@aql.transform(conn_id="snowflake_conn", task_id="transform_table")
def transform_table_func(query_table: Table):
    return """-- Write your SQL query here
SELECT NUM1 AS var1, NUM2 as var2, CLASS as TARGET FROM {{query_table}};"""

@aql.dataframe(task_id="random_forest")
def random_forest_func(transform_table: pd.DataFrame):
    # Write your code here...
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    
    my_dataframe = transform_table
    
    X = my_dataframe.iloc[:,:2]
    y = my_dataframe.iloc[:,2]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)
    
    model = RandomForestClassifier(max_depth=5, random_state=19)
    
    model.fit(X_train, y_train)
    
    prediction = model.predict(X_test)
    score = model.score(X_test, y_test)
    
    return prediction

@aql.transform(conn_id="", task_id="cell_1")
def cell_1_func():
    return """-- Write your SQL query here
-- Write your SQL query here
SELECT BREED, HEIGHT_LOW_INCHES, HEIGHT_HIGHT_INCHES, WEIGHT_LOW_LBS, WEIGHT_HIGH_LBS,
    CASE WHEN reps_upper <= 25 THEN 'very_smart_dog'
    ELSE 'smart_dog'
    END AS INTELLIGENCE_CATEGORY
FROM SANDBOX.TAMARAFINGERLIN.DOG_INTELLIGENCE
WHERE reps_lower IS NOT NULL AND reps_upper IS NOT NULL"""

@aql.dataframe(task_id="cell_2")
def cell_2_func(transform_table: pd.DataFrame):
    # Write your code here...
    # Write your code here...
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestClassifier
    
    # use the table returned from the transform_table cell
    my_dataframe = transform_table
    
    X = my_dataframe.iloc[:,1:-1]
    y = my_dataframe.iloc[:,-1]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)
    
    scaler = StandardScaler()
    
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)
    
    model = RandomForestClassifier(max_depth=3, random_state=19)
    
    model.fit(X_train_s, y_train)
    
    prediction = model.predict(X_test_s)
    score = model.score(X_test_s, y_test)
    
    return score

@aql.dataframe(task_id="cell_3")
def cell_3_func(transform_table: pd.DataFrame):
    # Write your code here...# Write your code here...
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    
    # use the table returned from the transform_table cell
    df = transform_table
    
    # calculate baseline accuracy
    baseline_accuracy = df.iloc[:,-1].value_counts(normalize=True)[0]
    
    # selecting predictors (X) and the target (y)
    X = df.iloc[:,:-1]
    y = df.iloc[:,-1]
    
    # split the data into training data (80%) and testing data (20%)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.20, random_state=23
    )
    
    # standardize features
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)
    
    # train a RandomForestClassifier on the training data
    model = LogisticRegression()
    model.fit(X_train_s, y_train)
    
    # score the trained model on the testing data
    score = model.score(X_test_s, y_test)
    
    
    
    return X_train.columns, model.coef_

@aql.transform(conn_id="snowflake_conn", task_id="query_table")
def query_table_func():
    return """-- Write your SQL query here
SELECT * FROM cloud_ide_tutorial;"""

@dag(
    schedule_interval=None,
    start_date=pendulum.from_format("2022-11-01", "YYYY-MM-DD"),
)
def cloud_ide_tutorial_pipeline():
    query_table = query_table_func()

    transform_table = transform_table_func(
        query_table,
    )

    random_forest = random_forest_func(
        transform_table,
    )

    cell_1 = cell_1_func()

    cell_2 = cell_2_func(
        transform_table,
    )

    cell_3 = cell_3_func(
        transform_table,
    )

    cell_2 << transform_table

    cell_3 << transform_table

    random_forest << transform_table

    transform_table << query_table

dag_obj = cloud_ide_tutorial_pipeline()
